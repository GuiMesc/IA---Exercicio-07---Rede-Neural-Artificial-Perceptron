{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Cópia de IA - Exercício - 06 - Rede Neural Artificial - Python - SkLearn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5tVff4dgKyg"
      },
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xB4G8ogKym"
      },
      "source": [
        "### Carregando Arquivo de Treinamento (.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5MRh5ZfgKyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55765150-4605-4ec4-efc1-96bf57a8f6c7"
      },
      "source": [
        "import pandas as pd\n",
        "# Carregando dados do arquivo CSV\n",
        "url = 'https://raw.githubusercontent.com/GuiMesc/IA---Exercicio-07---Rede-Neural-Artificial-Perceptron/main/diabetes.csv'\n",
        "base_Treinamento = pd.read_csv(url,sep=',', encoding = 'latin1').values\n",
        "print(\"---------------------------------\")\n",
        "print(\"Dados dos Pacientes - TREINAMENTO\")\n",
        "print(\"---------------------------------\")\n",
        "print(base_Treinamento)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# Extração dos Atributos a serem utilizadas pela rede\n",
        "print(\"Atributos de Entrada\")\n",
        "print(\"---------------------------------\")\n",
        "print(base_Treinamento[:, 0:8]) #numero de colunas do arquivo csv\n",
        "\n",
        "print(\"----------------------------\")\n",
        "print(\"Classificação Supervisionada\")\n",
        "print(\"----------------------------\")\n",
        "print(base_Treinamento[:, 8])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Dados dos Pacientes - TREINAMENTO\n",
            "---------------------------------\n",
            "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
            " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
            " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
            " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
            " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n",
            "---------------------------------\n",
            "Atributos de Entrada\n",
            "---------------------------------\n",
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
            "----------------------------\n",
            "Classificação Supervisionada\n",
            "----------------------------\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPpDXy7cgKyp"
      },
      "source": [
        "### Pré-processamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4nvyQJegKyq",
        "outputId": "0d27e182-f80c-4ccf-f2a3-5675e29e23c6"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Binarizador de rótulo\n",
        "lb = preprocessing.MinMaxScaler()\n",
        "\n",
        "#A saída da transformação é também conhecido como codificação 1-de-n\n",
        "#Transforma valores categóricos equidistantes em valores binários equidistantes.\n",
        "#Atributos categóricos com valores sim e não\n",
        "\n",
        "Pregnancies = lb.fit_transform(np.array(base_Treinamento[:,0]).reshape(-1,1))  \n",
        "\n",
        "Glucose = lb.fit_transform(np.array(base_Treinamento[:,1]).reshape(-1,1))  \n",
        "\n",
        "BloodPressure = lb.fit_transform(np.array(base_Treinamento[:,2]).reshape(-1,1))  \n",
        "\n",
        "SkinThickness = lb.fit_transform(np.array(base_Treinamento[:,3]).reshape(-1,1))  \n",
        "\n",
        "Insulin = lb.fit_transform(np.array(base_Treinamento[:,4]).reshape(-1,1))  \n",
        "\n",
        "BMI = lb.fit_transform(np.array(base_Treinamento[:,5]).reshape(-1,1))  \n",
        "\n",
        "DiabetesPedigreeFunction = lb.fit_transform(np.array(base_Treinamento[:,6]).reshape(-1,1))  \n",
        "\n",
        "Age = lb.fit_transform(np.array(base_Treinamento[:,7]).reshape(-1,1))  \n",
        "\n",
        "Outcome = lb.fit_transform(np.array(base_Treinamento[:,8]).reshape(-1,1))  \n",
        "\n",
        "#Concatenação de Atributos (Colunas) \n",
        "atributos_norm = np.column_stack((Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age))\n",
        "print(\"--------------------------------\")\n",
        "print(\"Atributos de Entrada - Numéricos\")\n",
        "print(\"--------------------------------\")\n",
        "print(atributos_norm)\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Classificação Supervisionada - Numéricos\")\n",
        "print(\"----------------------------------------\")\n",
        "diagnostico_norm = np.hstack((Outcome))\n",
        "print(diagnostico_norm)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Atributos de Entrada - Numéricos\n",
            "--------------------------------\n",
            "[[0.35294118 0.74371859 0.59016393 ... 0.50074516 0.23441503 0.48333333]\n",
            " [0.05882353 0.42713568 0.54098361 ... 0.39642325 0.11656704 0.16666667]\n",
            " [0.47058824 0.91959799 0.52459016 ... 0.34724292 0.25362938 0.18333333]\n",
            " ...\n",
            " [0.29411765 0.6080402  0.59016393 ... 0.390462   0.07130658 0.15      ]\n",
            " [0.05882353 0.63316583 0.49180328 ... 0.4485842  0.11571307 0.43333333]\n",
            " [0.05882353 0.46733668 0.57377049 ... 0.45305514 0.10119556 0.03333333]]\n",
            "----------------------------------------\n",
            "Classificação Supervisionada - Numéricos\n",
            "----------------------------------------\n",
            "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
            " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC4FUfD5gKyr"
      },
      "source": [
        "### Treinamento do Neurônio Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDmPLB5FgKys",
        "outputId": "ae52546b-c162-47ee-e10c-3735887d485b"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "# Treinamento do Perceptron a partir dos atributos de entrada e classificações\n",
        "modelo = Perceptron()\n",
        "modelo.fit(atributos_norm, diagnostico_norm)\n",
        "\n",
        "# Acurácia do modelo, que é : 1 - (predições erradas / total de predições)\n",
        "# Acurácia do modelo: indica uma performance geral do modelo. \n",
        "# Dentre todas as classificações, quantas o modelo classificou corretamente;\n",
        "# (VP+VN)/N\n",
        "print('Acurácia: %.3f' % modelo.score(atributos_norm, diagnostico_norm))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMwrAoNwgKyt"
      },
      "source": [
        "### ----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UKPw4jagKyu"
      },
      "source": [
        "# Validação do Aprendizado "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VweXD-8_gKyx"
      },
      "source": [
        "### Predição a partir de base de dados (.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceg0-DfVgKyx",
        "outputId": "deed476d-a2bf-4360-9924-d40791dbb243"
      },
      "source": [
        "import pandas as pd\n",
        "# Carregando dados do arquivo CSV\n",
        "url = 'https://raw.githubusercontent.com/GuiMesc/IA---Exercicio-07---Rede-Neural-Artificial-Perceptron/main/BaseTeste.csv'\n",
        "base_Testes = pd.read_csv(url,sep=',', encoding = 'latin1').values\n",
        "print(\"----------------------------\")\n",
        "print(\"Dados dos Pacientes - TESTES\")\n",
        "print(\"----------------------------\")\n",
        "print(base_Testes)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# Extração dos Atributos a serem utilizadas pela rede\n",
        "print(\"Atributos de Entrada\")\n",
        "print(\"---------------------------------\")\n",
        "print(base_Testes[:, 0:7])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------\n",
            "Dados dos Pacientes - TESTES\n",
            "----------------------------\n",
            "[[9.000e+00 1.000e+02 5.500e+01 2.700e+01 1.500e+02 3.150e+01 5.270e-01\n",
            "  4.000e+01]\n",
            " [5.000e+00 7.000e+01 6.300e+01 2.900e+01 1.000e+02 2.160e+01 1.730e-01\n",
            "  3.100e+01]\n",
            " [8.000e+00 1.830e+02 6.400e+01 0.000e+00 0.000e+00 2.330e+01 6.720e-01\n",
            "  2.000e+01]\n",
            " [0.000e+00 1.200e+02 6.000e+01 2.300e+01 9.400e+01 2.510e+01 1.500e-01\n",
            "  2.100e+01]\n",
            " [0.000e+00 1.100e+02 4.000e+01 3.500e+01 1.200e+02 4.310e+01 2.288e+00\n",
            "  3.000e+01]]\n",
            "---------------------------------\n",
            "Atributos de Entrada\n",
            "---------------------------------\n",
            "[[9.000e+00 1.000e+02 5.500e+01 2.700e+01 1.500e+02 3.150e+01 5.270e-01]\n",
            " [5.000e+00 7.000e+01 6.300e+01 2.900e+01 1.000e+02 2.160e+01 1.730e-01]\n",
            " [8.000e+00 1.830e+02 6.400e+01 0.000e+00 0.000e+00 2.330e+01 6.720e-01]\n",
            " [0.000e+00 1.200e+02 6.000e+01 2.300e+01 9.400e+01 2.510e+01 1.500e-01]\n",
            " [0.000e+00 1.100e+02 4.000e+01 3.500e+01 1.200e+02 4.310e+01 2.288e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6JQnLaEgKyx"
      },
      "source": [
        "### Pré-processamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJM8C3IDgKyy",
        "outputId": "556e340e-ab14-4346-86a9-534d282686f2"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Binarizador de rótulo\n",
        "lb = preprocessing.MinMaxScaler()\n",
        "\n",
        "#A saída da transformação é também conhecido como codificação 1-de-n\n",
        "#Transforma valores categóricos equidistantes em valores binários equidistantes.\n",
        "#Atributos categóricos com valores sim e não\n",
        "Pregnancies = lb.fit_transform(np.array(base_Treinamento[:,0]).reshape(-1,1))  \n",
        "\n",
        "Glucose = lb.fit_transform(np.array(base_Treinamento[:,1]).reshape(-1,1))  \n",
        "\n",
        "BloodPressure = lb.fit_transform(np.array(base_Treinamento[:,2]).reshape(-1,1))  \n",
        "\n",
        "SkinThickness = lb.fit_transform(np.array(base_Treinamento[:,3]).reshape(-1,1))  \n",
        "\n",
        "Insulin = lb.fit_transform(np.array(base_Treinamento[:,4]).reshape(-1,1))  \n",
        "\n",
        "BMI = lb.fit_transform(np.array(base_Treinamento[:,5]).reshape(-1,1))  \n",
        "\n",
        "DiabetesPedigreeFunction = lb.fit_transform(np.array(base_Treinamento[:,6]).reshape(-1,1))  \n",
        "\n",
        "Age = lb.fit_transform(np.array(base_Treinamento[:,7]).reshape(-1,1))\n",
        "\n",
        "#Concatenação de Atributos (Colunas) \n",
        "atributos_norm = np.column_stack((Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age))\n",
        "print(\"--------------------------------\")\n",
        "print(\"Atributos de Entrada - Numéricos\")\n",
        "print(\"--------------------------------\")\n",
        "print(atributos_norm)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Atributos de Entrada - Numéricos\n",
            "--------------------------------\n",
            "[[0.35294118 0.74371859 0.59016393 ... 0.50074516 0.23441503 0.48333333]\n",
            " [0.05882353 0.42713568 0.54098361 ... 0.39642325 0.11656704 0.16666667]\n",
            " [0.47058824 0.91959799 0.52459016 ... 0.34724292 0.25362938 0.18333333]\n",
            " ...\n",
            " [0.29411765 0.6080402  0.59016393 ... 0.390462   0.07130658 0.15      ]\n",
            " [0.05882353 0.63316583 0.49180328 ... 0.4485842  0.11571307 0.43333333]\n",
            " [0.05882353 0.46733668 0.57377049 ... 0.45305514 0.10119556 0.03333333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QO3B6S3gKyy"
      },
      "source": [
        "### Predição da Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZovGGdmgKyz",
        "outputId": "24733e9c-ead2-44e3-ce19-1a4ed930168e"
      },
      "source": [
        "base_Predicao = modelo.predict((atributos_norm))\n",
        "print(\"Classificações: \", base_Predicao)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classificações:  [1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0.\n",
            " 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
            " 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
            " 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.\n",
            " 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.\n",
            " 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
            " 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
            " 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0.\n",
            " 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}